{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostRegressor,  BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "import xgboost\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf  # for data\n",
    "from pandas_datareader import data as pdr\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(stock='AAPL', period = \"60d\", interval = \"15m\", intra = True):\n",
    "    '''\n",
    "    reading the data\n",
    "    '''\n",
    "\n",
    "    df = pdr.get_data_yahoo(stock,period = period,\n",
    "\n",
    "            # fetch data by interval (including intraday if period < 60 days)\n",
    "            # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "            # (optional, default is '1d')\n",
    "            interval = interval,)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_enhanced(stock='AAPL', period = \"60d\", interval = \"15m\", intra = True):\n",
    "    '''\n",
    "    reading the data\n",
    "    '''\n",
    "    \n",
    "    if intra ==True:\n",
    "\n",
    "\n",
    "        dfx = pdr.get_data_yahoo(stock,period = period,\n",
    "\n",
    "                # fetch data by interval (including intraday if period < 60 days)\n",
    "                # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "                # (optional, default is '1d')\n",
    "                interval = interval,).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "        df_nasaq = read_data(stock='NDAQ', period = period, interval = interval).reset_index()[['Datetime','Open','Adj Close']]\n",
    "\n",
    "        df_nasaq['fn'] = ((df_nasaq['Open'].shift(1)/df_nasaq['Adj Close'].shift(1))-1)\n",
    "        df_nasaq= df_nasaq[['Datetime','fn']]\n",
    "\n",
    "\n",
    "        df_sp = read_data(stock='^GSPC', period = period, interval = interval).reset_index()[['Datetime','Open','Adj Close']]\n",
    "        df_sp['fs'] = ((df_sp['Open'].shift(1)/df_sp['Adj Close'].shift(1))-1)\n",
    "        df_sp= df_sp[['Datetime','fs']]\n",
    "\n",
    "\n",
    "        dfx = pd.merge(dfx,df_nasaq,on = ['Datetime'], how = 'left')\n",
    "\n",
    "        dfx = pd.merge(dfx,df_sp,on = ['Datetime'], how = 'left')\n",
    "        \n",
    "    elif intra ==False:\n",
    "        dfx = pdr.get_data_yahoo(stock,period = period,\n",
    "\n",
    "        # fetch data by interval (including intraday if period < 60 days)\n",
    "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "        # (optional, default is '1d')\n",
    "        interval = interval,).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "        df_nasaq = read_data(stock='NDAQ', period = period, interval = interval).reset_index()[['Date','Open','Adj Close']]\n",
    "\n",
    "        df_nasaq['fn'] = ((df_nasaq['Open'].shift(1)/df_nasaq['Adj Close'].shift(1))-1)\n",
    "        df_nasaq= df_nasaq[['Date','fn']]\n",
    "\n",
    "\n",
    "        df_sp = read_data(stock='^GSPC', period = period, interval = interval).reset_index()[['Date','Open','Adj Close']]\n",
    "        df_sp['fs'] = ((df_sp['Open'].shift(1)/df_sp['Adj Close'].shift(1))-1)\n",
    "        df_sp= df_sp[['Date','fs']]\n",
    "\n",
    "\n",
    "        dfx = pd.merge(dfx,df_nasaq,on = ['Date'], how = 'left')\n",
    "\n",
    "        dfx = pd.merge(dfx,df_sp,on = ['Date'], how = 'left')\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df1 = read_data(stock='AAPL').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-12 09:30:00-04:00</td>\n",
       "      <td>120.059998</td>\n",
       "      <td>120.150002</td>\n",
       "      <td>119.284500</td>\n",
       "      <td>119.690002</td>\n",
       "      <td>119.690002</td>\n",
       "      <td>11041255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-12 09:32:00-04:00</td>\n",
       "      <td>119.699997</td>\n",
       "      <td>120.150002</td>\n",
       "      <td>119.400002</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>2273540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-12 09:34:00-04:00</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.680000</td>\n",
       "      <td>119.910004</td>\n",
       "      <td>120.559998</td>\n",
       "      <td>120.559998</td>\n",
       "      <td>2563680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-12 09:36:00-04:00</td>\n",
       "      <td>120.750000</td>\n",
       "      <td>120.910004</td>\n",
       "      <td>120.750000</td>\n",
       "      <td>120.906601</td>\n",
       "      <td>120.906601</td>\n",
       "      <td>2064244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-12 09:38:00-04:00</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>120.581596</td>\n",
       "      <td>120.762299</td>\n",
       "      <td>120.762299</td>\n",
       "      <td>2285976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime        Open        High         Low       Close  \\\n",
       "0 2020-10-12 09:30:00-04:00  120.059998  120.150002  119.284500  119.690002   \n",
       "1 2020-10-12 09:32:00-04:00  119.699997  120.150002  119.400002  120.000000   \n",
       "2 2020-10-12 09:34:00-04:00  120.000000  120.680000  119.910004  120.559998   \n",
       "3 2020-10-12 09:36:00-04:00  120.750000  120.910004  120.750000  120.906601   \n",
       "4 2020-10-12 09:38:00-04:00  121.000000  121.000000  120.581596  120.762299   \n",
       "\n",
       "    Adj Close    Volume  \n",
       "0  119.690002  11041255  \n",
       "1  120.000000   2273540  \n",
       "2  120.559998   2563680  \n",
       "3  120.906601   2064244  \n",
       "4  120.762299   2285976  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is the tail of the df which has the most recent data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>fn</th>\n",
       "      <th>fs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-12 09:30:00-04:00</td>\n",
       "      <td>120.059998</td>\n",
       "      <td>120.150002</td>\n",
       "      <td>119.284500</td>\n",
       "      <td>119.690002</td>\n",
       "      <td>119.690002</td>\n",
       "      <td>11041255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-12 09:32:00-04:00</td>\n",
       "      <td>119.699997</td>\n",
       "      <td>120.150002</td>\n",
       "      <td>119.400002</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>2273540</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-12 09:34:00-04:00</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.680000</td>\n",
       "      <td>119.910004</td>\n",
       "      <td>120.559998</td>\n",
       "      <td>120.559998</td>\n",
       "      <td>2563680</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>-0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-12 09:36:00-04:00</td>\n",
       "      <td>120.750000</td>\n",
       "      <td>120.910004</td>\n",
       "      <td>120.750000</td>\n",
       "      <td>120.906601</td>\n",
       "      <td>120.906601</td>\n",
       "      <td>2064244</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>-0.000822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-12 09:38:00-04:00</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>120.581596</td>\n",
       "      <td>120.762299</td>\n",
       "      <td>120.762299</td>\n",
       "      <td>2285976</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.000585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime        Open        High         Low       Close  \\\n",
       "0 2020-10-12 09:30:00-04:00  120.059998  120.150002  119.284500  119.690002   \n",
       "1 2020-10-12 09:32:00-04:00  119.699997  120.150002  119.400002  120.000000   \n",
       "2 2020-10-12 09:34:00-04:00  120.000000  120.680000  119.910004  120.559998   \n",
       "3 2020-10-12 09:36:00-04:00  120.750000  120.910004  120.750000  120.906601   \n",
       "4 2020-10-12 09:38:00-04:00  121.000000  121.000000  120.581596  120.762299   \n",
       "\n",
       "    Adj Close    Volume        fn        fs  \n",
       "0  119.690002  11041255       NaN       NaN  \n",
       "1  120.000000   2273540 -0.000079 -0.000149  \n",
       "2  120.559998   2563680  0.001988 -0.000263  \n",
       "3  120.906601   2064244 -0.002065 -0.000822  \n",
       "4  120.762299   2285976  0.000040 -0.000585  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = read_data_enhanced(stock='AAPL')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the follwoing model already established"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stock(dfx,intra =True):\n",
    "    \"\"\"\n",
    "    predict the price for the closing price for the bext inerval. \n",
    "    \n",
    "    inputs:\n",
    "    ------\n",
    "    - stock: string, symobol for the stick, eg. 'TSLA'\n",
    "    - intra : Boolean, if True then the period and interval should be 1m, 5m, etc, if False it should be 1d, 7d, etc\n",
    "    - period : total period\n",
    "    - interval : interval for the data\n",
    "    \n",
    "    \"\"\"\n",
    "    df = dfx.copy()\n",
    "    #df['day_of_the_week'] = df['Datetime'].dt.dayofweek\n",
    "    #df['day_of_month'] = df['Datetime'].apply(lambda x:int(str(x)[8:10]))\n",
    "\n",
    "\n",
    "    \n",
    "    #print(len(df))\n",
    "    print(df.tail(1)['Datetime'])\n",
    "    old = df.tail(1).loc[:,'Open'].to_numpy()[0]\n",
    "    \n",
    "    if intra ==True:\n",
    "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "            df['Datetime']=df['Datetime'].map(dt.datetime.toordinal)\n",
    "            df['SMA_1'] = df['Adj Close'].rolling(window = 26).mean()\n",
    "            df['SMA_2'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "            df['status_sma_lower'] = np.where(df['Adj Close'] < df['SMA_1'],'below_movavg','normal')\n",
    "            df['status_sma_upper'] = np.where(df['Adj Close'] > df['SMA_2'],'above_movavg','normal')\n",
    "    \n",
    "    elif intra ==False:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Date']=df['Date'].map(dt.datetime.toordinal)\n",
    "#             df['SMA_1'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "#             df['SMA_2'] = df['Adj Close'].rolling(window = 100).mean()\n",
    "#             df['status_sma_lower'] = np.where(df['Adj Close'] < df['SMA_1'],'below_movavg','normal')\n",
    "\n",
    "    \n",
    "\n",
    "     \n",
    "    df['High'] = df['High'].shift(1)\n",
    "    df['Low'] = df['Low'].shift(1)\n",
    "    df['Volume'] = df['Volume'].shift(1)\n",
    "    \n",
    "    df['Middle Band'] =df['Adj Close'].rolling(window=26).mean()\n",
    "    df['Upper Band'] = df['Middle Band'] + 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['Lower Band'] = df['Middle Band'] - 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['status_lower'] = np.where(df['Adj Close'] < df['Lower Band'],'below_ballinger','normal')\n",
    "    df['status_upper'] = np.where(df['Adj Close'] > df['Upper Band'],'above_ballinger','normal')\n",
    "    #df['f01'] = df['Close']/df['Open']-1\n",
    "\n",
    "\n",
    "    df = df.dropna()\n",
    "    df = df.drop(columns = ['Close'])\n",
    "    y = df['Adj Close']\n",
    "    X = df.drop(columns =['Adj Close','Volume' ,'Low','High'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle= False)\n",
    "        # first transformer for the numeric features\n",
    "    numeric_features = list(set(list(X_train.columns)) - set(['status_lower','status_upper','status_sma_lower','status_sma_upper']))\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "       ])\n",
    "    # now a taransformer for the categorical features\n",
    "    categorical_features = ['status_lower','status_upper']\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    # creating a preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    ridge_model = Ridge()\n",
    "    # include the preprocessor and the model in one pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('Regressor', ridge_model)])\n",
    "\n",
    "    # finally we will pass the pipe line to gridsearchcv to find the optimum paramters for the model\n",
    "    param_grid = {\n",
    "        'Regressor__alpha':[0.1,0.25,0.4],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    search = GridSearchCV(reg_pipeline,param_grid,cv = 5)\n",
    "\n",
    "    # fitting the model\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # printing the first parameter\n",
    "    #rint(search.best_params_)\n",
    "    print(\"model score: %.3f\" % search.score(X_test, y_test))\n",
    "    X_train = X[:-1]\n",
    "    y_train = y[:-1]\n",
    "    X_test = X[-1:]\n",
    "    y_test = y[-1:]\n",
    "    search.fit(X_train,y_train)\n",
    "    y_pred = search.predict(X_train)\n",
    "    print('error',np.sqrt(mean_squared_error(y_train,y_pred)))\n",
    "    y_test = search.predict(X_test)[0]\n",
    "    diff = y_test - old\n",
    "   \n",
    "    print('old price = ',old)\n",
    "    print('predicted = ',y_test)\n",
    "    print('difference = ', diff)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see the power of feature engineering:\n",
    "#we will add the daily return as a feature for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stock_w_feature(dfx,intra =True):\n",
    "    \"\"\"\n",
    "    predict the price for the closing price for the bext inerval. \n",
    "    \n",
    "    inputs:\n",
    "    ------\n",
    "    - stock: string, symobol for the stick, eg. 'TSLA'\n",
    "    - intra : Boolean, if True then the period and interval should be 1m, 5m, etc, if False it should be 1d, 7d, etc\n",
    "    - period : total period\n",
    "    - interval : interval for the data\n",
    "    \n",
    "    \"\"\"\n",
    "    #print(len(df))\n",
    "\n",
    "\n",
    "    df = dfx.copy()\n",
    "    \n",
    "    # for now the following do not add any predictiv power\n",
    "    #df['day_of_the_week'] = df['Datetime'].dt.dayofweek\n",
    "    #df['day_of_month'] = df['Datetime'].apply(lambda x:int(str(x)[8:10]))\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    df['f01'] = ((df['Open']/df['Close'].shift(1))-1)*100\n",
    "    df['O-1'] = df['Open'].shift(1)\n",
    "    df['O-2'] = df['Open'].shift(2)\n",
    "    df['O-3'] = df['Open'].shift(3)\n",
    "    df['O-4'] = df['Open'].shift(4)\n",
    "    df['O-5'] = df['Open'].shift(5)\n",
    "    \n",
    "    df['c-1'] = df['Close'].shift(1)\n",
    "    df['c-2'] = df['Close'].shift(2)\n",
    "    df['c-3'] = df['Close'].shift(3)\n",
    "    df['c-4'] = df['Close'].shift(4)\n",
    "    df['c-5'] = df['Close'].shift(5)\n",
    "    \n",
    "    \n",
    "    old = df.tail(1).loc[:,'Open'].to_numpy()[0]\n",
    "    \n",
    "    if intra ==True:\n",
    "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "            df['Datetime']=df['Datetime'].map(dt.datetime.toordinal)\n",
    "            df['SMA_1'] = df['Adj Close'].rolling(window = 26).mean()\n",
    "            df['SMA_2'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "            df['status_sma_lower'] = np.where(df['Adj Close'] < df['SMA_1'],'below_movavg','normal')\n",
    "            df['status_sma_upper'] = np.where(df['Adj Close'] > df['SMA_2'],'above_movavg','normal')\n",
    "    \n",
    "    elif intra ==False:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Date']=df['Date'].map(dt.datetime.toordinal)\n",
    "#             df['SMA_1'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "#             df['SMA_2'] = df['Adj Close'].rolling(window = 100).mean()\n",
    "#             df['status_sma_lower'] = np.where(df['Adj Close'] < df['SMA_1'],'below_movavg','normal')\n",
    "#             df['status_sma_upper'] = np.where(df['Adj Close'] > df['SMA_2'],'above_movavg','normal')\n",
    "\n",
    "     \n",
    "    df['High'] = df['High'].shift(1)\n",
    "    df['Low'] = df['Low'].shift(1)\n",
    "    df['Volume'] = df['Volume'].shift(1)\n",
    "    df['Middle Band'] =df['Adj Close'].rolling(window=26).mean()\n",
    "    df['Upper Band'] = df['Middle Band'] + 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['Lower Band'] = df['Middle Band'] - 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['status_lower'] = np.where(df['Adj Close'] < df['Lower Band'],'below_ballinger','normal')\n",
    "    df['status_upper'] = np.where(df['Adj Close'] > df['Upper Band'],'above_ballinger','normal')\n",
    "    #df['f01'] = df['Close']/df['Open']-1\n",
    "\n",
    "\n",
    "    df = df.dropna()\n",
    "    df = df.drop(columns = ['Close'])\n",
    "    y = df['Adj Close']\n",
    "    X = df.drop(columns =['Adj Close','Volume' ,'Low','High'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle= False)\n",
    "        # first transformer for the numeric features\n",
    "    numeric_features = list(set(list(X_train.columns)) - set(['status_lower','status_upper','status_sma_lower','status_sma_upper']))\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "       ])\n",
    "    # now a taransformer for the categorical features\n",
    "    categorical_features = ['status_lower','status_upper']\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    # creating a preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    ridge_model = Ridge()\n",
    "    # include the preprocessor and the model in one pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('Regressor', ridge_model)])\n",
    "\n",
    "    # finally we will pass the pipe line to gridsearchcv to find the optimum paramters for the model\n",
    "    param_grid = {\n",
    "        'Regressor__alpha':[0.1,0.25,0.4],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    search = GridSearchCV(reg_pipeline,param_grid,cv = 5)\n",
    "\n",
    "    # fitting the model\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # printing the first parameter\n",
    "    #rint(search.best_params_)\n",
    "    print(\"model score: %.3f\" % search.score(X_test, y_test))\n",
    "    X_train = X[:-1]\n",
    "    y_train = y[:-1]\n",
    "    X_test = X[-1:]\n",
    "    y_test = y[-1:]\n",
    "    search.fit(X_train,y_train)\n",
    "    y_pred = search.predict(X_train)\n",
    "    print('error',np.sqrt(mean_squared_error(y_train,y_pred)))\n",
    "    y_test = search.predict(X_test)[0]\n",
    "    diff = y_test - old\n",
    "   \n",
    "    print('old price = ',old)\n",
    "    print('predicted = ',y_test)\n",
    "    print('difference = ', diff)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6303   2020-11-25 11:38:34-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "model score: 0.998\n",
      "error 0.14073852965767322\n",
      "old price =  116.11730194091797\n",
      "predicted =  116.14334753121437\n",
      "difference =  0.026045590296405408\n"
     ]
    }
   ],
   "source": [
    "# let's predict he clost price for TSLA after 5 minutes\n",
    "test_stock(df1, intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.998\n",
      "error 0.13902068205813273\n",
      "old price =  116.11730194091797\n",
      "predicted =  116.14578795972683\n",
      "difference =  0.028486018808862923\n"
     ]
    }
   ],
   "source": [
    "test_stock_w_feature(df1, intra = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With NASDAQ dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6303   2020-11-25 11:38:34-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "model score: 0.998\n",
      "error 0.14052909034976063\n",
      "old price =  116.11730194091797\n",
      "predicted =  116.2861032081868\n",
      "difference =  0.16880126726883304\n"
     ]
    }
   ],
   "source": [
    "# let's predict he clost price for TSLA after 5 minutes\n",
    "test_stock(df2, intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.998\n",
      "error 0.13911352723207823\n",
      "old price =  116.11730194091797\n",
      "predicted =  116.27391523913255\n",
      "difference =  0.15661329821458025\n"
     ]
    }
   ],
   "source": [
    "test_stock_w_feature(df2, intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we same the same gain if use a different modeel: Random Forest ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stock_grad(dfx, intra = True):\n",
    "    \"\"\"\n",
    "    predict the price for the closing price for the bext inerval. \n",
    "    \n",
    "    inputs:\n",
    "    ------\n",
    "    - stock: string, symobol for the stick, eg. 'TSLA'\n",
    "    - intra : Boolean, if True then the period and interval should be 1m, 5m, etc, if False it should be 1d, 7d, etc\n",
    "    - period : total period\n",
    "    - interval : interval for the data\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df = dfx.copy()\n",
    "    df['day_of_the_week'] = df['Datetime'].dt.dayofweek\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #print(len(df))\n",
    "    print(df.tail(1)['Datetime'])\n",
    "    old = df.tail(1).loc[:,'Open'].to_numpy()[0]\n",
    "    \n",
    "    if intra ==True:\n",
    "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "            df['Datetime']=df['Datetime'].map(dt.datetime.toordinal)\n",
    "            df['SMA_1'] = df['Adj Close'].rolling(window = 26).mean()\n",
    "            df['SMA_2'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "            df['status_sma_lower'] = np.where(df['Adj Close'] < df['SMA_1'],'below_movavg','normal')\n",
    "            df['status_sma_upper'] = np.where(df['Adj Close'] > df['SMA_2'],'above_movavg','normal')\n",
    "    \n",
    "    elif intra ==False:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Date']=df['Date'].map(dt.datetime.toordinal)\n",
    "#             df['SMA_1'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "#             df['SMA_2'] = df['Adj Close'].rolling(window = 100).mean()\n",
    "#             df['status_sma_lower'] = np.where(df['Adj Close'] < df['SMA_1'],'below_movavg','normal')\n",
    "#             df['status_sma_upper'] = np.where(df['Adj Close'] > df['SMA_2'],'above_movavg','normal')\n",
    "\n",
    "     \n",
    "    df['High'] = df['High'].shift(1)\n",
    "    df['Low'] = df['Low'].shift(1)\n",
    "    df['Volume'] = df['Volume'].shift(1) \n",
    "    df['Middle Band'] =df['Adj Close'].rolling(window=26).mean()\n",
    "    df['Upper Band'] = df['Middle Band'] + 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['Lower Band'] = df['Middle Band'] - 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['status_lower'] = np.where(df['Adj Close'] < df['Lower Band'],'below_ballinger','normal')\n",
    "    df['status_upper'] = np.where(df['Adj Close'] > df['Upper Band'],'above_ballinger','normal')\n",
    "    df = df.dropna()\n",
    "    df = df.drop(columns = ['Close'])\n",
    "    y = df['Adj Close']\n",
    "    X = df.drop(columns =['Adj Close','Volume' ,'Low','High'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle= False, random_state = 2020)\n",
    "        # first transformer for the numeric features\n",
    "    numeric_features = list(set(list(X_train.columns)) - set(['status_lower','status_upper','status_sma_lower','status_sma_upper']))\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "       ])\n",
    "    # now a taransformer for the categorical features\n",
    "    categorical_features = ['status_lower']\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    # creating a preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    ridge_model = RandomForestRegressor(random_state = 2020)\n",
    "    # include the preprocessor and the model in one pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('Regressor', ridge_model)])\n",
    "\n",
    "    # finally we will pass the pipe line to gridsearchcv to find the optimum paramters for the model\n",
    "    param_grid = {\n",
    "        'Regressor__n_estimators':[100],\n",
    "    }\n",
    "    search = GridSearchCV(reg_pipeline,param_grid,cv = 5)\n",
    "\n",
    "    # fitting the model\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # printing the first parameter\n",
    "    #rint(search.best_params_)\n",
    "    print(\"model score: %.3f\" % search.score(X_test, y_test))\n",
    "        #search.fit(X_train,y_train)\n",
    "    y_pred = search.predict(X_test)\n",
    "    error = np.sqrt(mean_squared_error(y_pred,y_test))\n",
    "    print('error',error)\n",
    "#     X_train = X[:-1]\n",
    "#     y_train = y[:-1]\n",
    "    X_test = X[-1:]\n",
    "    y_test = y[-1:]\n",
    "    y_test = search.predict(X_test)[0]\n",
    "    diff = y_test - old\n",
    "        \n",
    "    if diff > error :\n",
    "        print('winner')\n",
    "        print('old price = ',old)\n",
    "        print('predicted = ',y_test)\n",
    "        print('difference = ', diff)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stock_grad_feature(dfx, intra = True):\n",
    "    \"\"\"\n",
    "    predict the price for the closing price for the bext inerval. \n",
    "    \n",
    "    inputs:\n",
    "    ------\n",
    "    - stock: string, symobol for the stick, eg. 'TSLA'\n",
    "    - intra : Boolean, if True then the period and interval should be 1m, 5m, etc, if False it should be 1d, 7d, etc\n",
    "    - period : total period\n",
    "    - interval : interval for the data\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df = dfx.copy()\n",
    "    df['M7'] =df['Adj Close'].rolling(window=7).mean()\n",
    "    df['M7d'] =df['Adj Close'].rolling(window=7).std()\n",
    "    df['M14'] =df['Adj Close'].rolling(window=14).mean()\n",
    "    df['M14d'] =df['Adj Close'].rolling(window=14).std()\n",
    "    df['M21'] =df['Adj Close'].rolling(window=21).mean()\n",
    "    df['M21d'] =df['Adj Close'].rolling(window=21).std()\n",
    "\n",
    "\n",
    "    df['f01'] = ((df['Open'].shift(1)/df['Adj Close'].shift(1))-1)\n",
    "    df['f02'] = ((df['Open'].shift(2)/df['Adj Close'].shift(2))-1)\n",
    "    df['f03'] = ((df['Open'].shift(3)/df['Adj Close'].shift(3))-1)\n",
    "    df['v01'] = ((df['Volume'].shift(1)/df['Volume'].shift(1))-1)\n",
    "#     df['f02'] = ((df['Open'].shift(1)/df['Adj Close'].shift(2))-1)*100\n",
    "#     df['O-1'] = df['Open'].shift(1)\n",
    "#     df['O-2'] = df['Open'].shift(2)\n",
    "#     df['O-3'] = df['Open'].shift(3)\n",
    "#     df['O-4'] = df['Open'].shift(4)\n",
    "#     df['O-5'] = df['Open'].shift(5)\n",
    "    \n",
    "#     df['c-1'] = df['Close'].shift(1)\n",
    "#     df['c-2'] = df['Close'].shift(2)\n",
    "#     df['c-3'] = df['Close'].shift(3)\n",
    "#     df['c-4'] = df['Close'].shift(4)\n",
    "#     df['c-5'] = df['Close'].shift(5)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #print(len(df))\n",
    "\n",
    "    \n",
    "    if intra ==True:\n",
    "            print(df.tail(1)['Datetime'])\n",
    "            old = df.tail(1).loc[:,'Open'].to_numpy()[0]\n",
    "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "            \n",
    "            \n",
    "            df['day_of_the_week'] = df['Datetime'].dt.dayofweek\n",
    "            df['day_of_month'] = df['Datetime'].apply(lambda x:int(str(x)[8:10]))\n",
    "            df['Datetime']=df['Datetime'].map(dt.datetime.toordinal)\n",
    "            \n",
    "            df['SMA_1'] = df['Adj Close'].rolling(window = 26).mean()\n",
    "            df['SMA_2'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "            df['status_sma_lower'] = np.where(df['Adj Close'] < df['SMA_1'],'below_movavg','normal')\n",
    "            df['status_sma_upper'] = np.where(df['Adj Close'] > df['SMA_2'],'above_movavg','normal')\n",
    "    \n",
    "    elif intra ==False:\n",
    "            print(df.tail(1)['Date'])\n",
    "            old = df.tail(1).loc[:,'Open'].to_numpy()[0]\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            df['day_of_the_week'] = df['Date'].dt.dayofweek\n",
    "            df['day_of_month'] = df['Date'].apply(lambda x:int(str(x)[8:10]))\n",
    "            df['Date']=df['Date'].map(dt.datetime.toordinal)\n",
    "            \n",
    "            df['SMA_1'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "            df['SMA_2'] = df['Adj Close'].rolling(window = 100).mean()\n",
    "            df['status_sma_lower'] = np.where(df['Adj Close'] < df['SMA_1'],'below_movavg','normal')\n",
    "            df['status_sma_upper'] = np.where(df['Adj Close'] > df['SMA_2'],'above_movavg','normal')\n",
    "\n",
    "     \n",
    "    df['High'] = df['High'].shift(1)\n",
    "    df['Low'] = df['Low'].shift(1)\n",
    "    df['Volume'] = df['Volume'].shift(1) \n",
    "    df['Middle Band'] =df['Adj Close'].rolling(window=26).mean()\n",
    "    df['Upper Band'] = df['Middle Band'] + 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['Lower Band'] = df['Middle Band'] - 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['status_lower'] = np.where(df['Adj Close'] < df['Lower Band'],'below_ballinger','normal')\n",
    "    df['status_upper'] = np.where(df['Adj Close'] > df['Upper Band'],'above_ballinger','normal')\n",
    "    df = df.dropna()\n",
    "    df = df.drop(columns = ['Close'])\n",
    "    y = df['Adj Close']\n",
    "    X = df.drop(columns =['Adj Close','Volume' ,'Low','High'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle= False, random_state = 2020)\n",
    "        # first transformer for the numeric features\n",
    "    numeric_features = list(set(list(X_train.columns)) - set(['status_lower','status_upper','status_sma_lower','status_sma_upper']))\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "       ])\n",
    "    # now a taransformer for the categorical features\n",
    "    categorical_features = ['status_lower','status_upper','status_sma_lower','status_sma_upper']\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    # creating a preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    ridge_mode = RandomForestRegressor(random_state = 2020, n_estimators = 2, n_jobs = 24)\n",
    "    # include the preprocessor and the model in one pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    reg_pipelin = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('Regressor', ridge_mode)])\n",
    "\n",
    "    # finally we will pass the pipe line to gridsearchcv to find the optimum paramters for the model\n",
    "#     param_grid ={'Regressor__bootstrap': [False], 'Regressor__max_depth': [10,20,50],\n",
    "#                  'Regressor__min_samples_leaf': [1,4,6], 'Regressor__min_samples_split': [5,10],\n",
    "#                  'Regressor__n_estimators': [10,50]}\n",
    "    \n",
    "    param_grid ={'Regressor__n_estimators': [200],\n",
    "                 'Regressor__max_depth': [15]\n",
    "                \n",
    "                }\n",
    "    \n",
    "    \n",
    "    search = GridSearchCV(reg_pipelin,param_grid,cv = 5)\n",
    "\n",
    "    # fitting the model\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # printing the first parameter\n",
    "    print(search.best_params_)\n",
    "    print(\"model training score: %.3f\" % search.score(X_train, y_train))\n",
    "    print(\"model test score: %.3f\" % search.score(X_test, y_test))\n",
    "    \n",
    "    # training the model on the whole data set\n",
    "\n",
    "    #search.fit(X_train,y_train)\n",
    "    y_pred = search.predict(X_test)\n",
    "    error = np.sqrt(mean_squared_error(y_pred,y_test))\n",
    "    print('error',error)\n",
    "#     X_train = X[:-1]\n",
    "#     y_train = y[:-1]\n",
    "    X_test = X[-1:]\n",
    "    y_test = y[-1:]\n",
    "    y_test = search.predict(X_test)[0]\n",
    "    diff = y_test - old\n",
    "        \n",
    "    if diff > error :\n",
    "        print('winner')\n",
    "        print('old price = ',old)\n",
    "        print('predicted = ',y_test)\n",
    "        print('difference = ', diff)\n",
    "    print('old price = ',old)\n",
    "    print('predicted = ',y_test)\n",
    "    print('difference = ', diff)\n",
    "    \n",
    "    return error\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df_2 = read_data(stock='AAPL',interval = \"2m\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df_5 = read_data(stock='AAPL',interval = \"5m\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df_15 = read_data(stock='AAPL',interval = \"15m\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df_30 = read_data(stock='AAPL',interval = \"30m\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df_60 = read_data(stock='AAPL',interval = \"60m\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df_360 = read_data(stock='AAPL',period = \"200d\",interval = \"1d\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "dfe_2 = read_data_enhanced(stock='AAPL',interval = \"2m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's conduct a sensitivity study for the effect of interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6336   2020-11-25 12:43:08-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "{'Regressor__max_depth': 20, 'Regressor__n_estimators': 150}\n",
      "model training score: 1.000\n",
      "model test score: 0.998\n",
      "error 0.10014484025552677\n",
      "old price =  115.96499633789062\n",
      "predicted =  116.00634801228841\n",
      "difference =  0.041351674397787974\n"
     ]
    }
   ],
   "source": [
    "# let's predict he clost price for TSLA after 2 minutes\n",
    "test_stock_grad_feature(df_2,intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4641   2020-11-25 12:43:21-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "{'Regressor__max_depth': 20, 'Regressor__n_estimators': 150}\n",
      "model training score: 0.999\n",
      "model test score: 0.985\n",
      "error 0.22299104049349214\n",
      "old price =  115.94499969482422\n",
      "predicted =  115.9198316040039\n",
      "difference =  -0.02516809082031557\n"
     ]
    }
   ],
   "source": [
    "test_stock_grad_feature(df_5,intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1547   2020-11-25 12:43:32-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "{'Regressor__max_depth': 20, 'Regressor__n_estimators': 150}\n",
      "model training score: 0.998\n",
      "model test score: 0.943\n",
      "error 0.43089306398385796\n",
      "old price =  115.9574966430664\n",
      "predicted =  116.10355144924586\n",
      "difference =  0.14605480617944977\n"
     ]
    }
   ],
   "source": [
    "test_stock_grad_feature(df_15,intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773   2020-11-25 12:30:00-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "{'Regressor__max_depth': 20, 'Regressor__n_estimators': 150}\n",
      "model training score: 0.997\n",
      "model test score: 0.909\n",
      "error 0.5553211694495666\n",
      "old price =  115.94000244140625\n",
      "predicted =  115.9382512919108\n",
      "difference =  -0.0017511494954476348\n"
     ]
    }
   ],
   "source": [
    "test_stock_grad_feature(df_30,intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417   2020-11-25 12:45:45-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "{'Regressor__max_depth': 20, 'Regressor__n_estimators': 150}\n",
      "model training score: 0.995\n",
      "model test score: 0.864\n",
      "error 0.7100651564487155\n",
      "old price =  116.02999877929688\n",
      "predicted =  116.07517801920572\n",
      "difference =  0.045179239908847535\n"
     ]
    }
   ],
   "source": [
    "test_stock_grad_feature(df_60,intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199   2020-11-25\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "{'Regressor__max_depth': 20, 'Regressor__n_estimators': 150}\n",
      "model training score: 0.992\n",
      "model test score: 0.654\n",
      "error 1.891168957823174\n",
      "old price =  115.55000305175781\n",
      "predicted =  114.87022313435872\n",
      "difference =  -0.6797799173990882\n"
     ]
    }
   ],
   "source": [
    "test_stock_grad_feature(df_360,intra = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we increases the time interval, we have less data and hence less accuracy\n",
    "> Once we went beyond the intra to full day, then we have error > 1 and sometimes the score is negative deponding on how much data we included, it is very senstive in this regard. Not stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's conduct a sensitivity study for the effect of enhanced data frame with other stocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "dfe_2 = read_data_enhanced(stock='AAPL',period = \"60d\",interval = \"2m\", intra = True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Using the df2 improved th result by 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6354   2020-11-25 13:20:07-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "{'Regressor__max_depth': 20, 'Regressor__n_estimators': 150}\n",
      "model training score: 1.000\n",
      "model test score: 0.998\n",
      "error 0.10382104098005492\n",
      "old price =  116.14929962158203\n",
      "predicted =  116.24587849934896\n",
      "difference =  0.0965788777669303\n"
     ]
    }
   ],
   "source": [
    "test_stock_grad_feature(dfe_2,intra = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> reducing the n_estimators seems to increase overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> df2 did not improve the accuracy for the 2 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "dfe_360 = read_data_enhanced(stock='AAPL',period = \"200d\",interval = \"1d\", intra = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199   2020-11-25\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "{'Regressor__max_depth': 20, 'Regressor__n_estimators': 150}\n",
      "model training score: 0.992\n",
      "model test score: 0.672\n",
      "error 1.8416963107101247\n",
      "old price =  115.55000305175781\n",
      "predicted =  114.73751286824545\n",
      "difference =  -0.8124901835123666\n"
     ]
    }
   ],
   "source": [
    "test_stock_grad_feature(dfe_360,intra = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we see some improvements for the daily predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's trade. 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks = ['AAL', 'BAC','AMD','PLUG','WORK']\n",
    "\n",
    "# for stock in stocks :\n",
    "#     df_2 = read_data(stock=stock,interval = \"2m\").reset_index()\n",
    "#     error = test_stock_grad_feature(df_2,intra = True)\n",
    "#     print(stock,error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df_2 = read_data(stock='AMD',interval = \"5m\").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4664   2020-11-25 14:39:49-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "{'Regressor__max_depth': 15, 'Regressor__n_estimators': 200}\n",
      "model training score: 0.999\n",
      "model test score: 0.990\n",
      "error 0.22073264691875563\n",
      "old price =  86.9800033569336\n",
      "predicted =  87.08065473556519\n",
      "difference =  0.10065137863159634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22073264691875563"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stock_grad_feature(df_2,intra = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming this into a classification Taks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df_trial = read_data(stock='AMD',interval = \"5m\").reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stock_grad_classify(dfx, intra = True):\n",
    "    \"\"\"\n",
    "    predict the price for the closing price for the bext inerval. \n",
    "    \n",
    "    inputs:\n",
    "    ------\n",
    "    - stock: string, symobol for the stick, eg. 'TSLA'\n",
    "    - intra : Boolean, if True then the period and interval should be 1m, 5m, etc, if False it should be 1d, 7d, etc\n",
    "    - period : total period\n",
    "    - interval : interval for the data\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df = dfx.copy()\n",
    "    df['tomorrow_closing'] = df['Adj Close'].shift(-1)\n",
    "    df = df.dropna()\n",
    "    df['go_up'] = np.where(df['tomorrow_closing'] > df['Adj Close'],1,0)\n",
    "    \n",
    "    df['M7'] =df['Adj Close'].rolling(window=7).mean()\n",
    "    df['M7d'] =df['Adj Close'].rolling(window=7).std()\n",
    "    df['M14'] =df['Adj Close'].rolling(window=14).mean()\n",
    "    df['M14d'] =df['Adj Close'].rolling(window=14).std()\n",
    "    df['M21'] =df['Adj Close'].rolling(window=21).mean()\n",
    "    df['M21d'] =df['Adj Close'].rolling(window=21).std()\n",
    "\n",
    "\n",
    "    df['f01'] = ((df['Open'].shift(1)/df['Adj Close'].shift(1))-1)\n",
    "    df['f02'] = ((df['Open'].shift(2)/df['Adj Close'].shift(2))-1)\n",
    "    df['f03'] = ((df['Open'].shift(3)/df['Adj Close'].shift(3))-1)\n",
    "    df['v01'] = ((df['Volume'].shift(1)/df['Volume'].shift(1))-1)\n",
    "#     df['f02'] = ((df['Open'].shift(1)/df['Adj Close'].shift(2))-1)*100\n",
    "#     df['O-1'] = df['Open'].shift(1)\n",
    "#     df['O-2'] = df['Open'].shift(2)\n",
    "#     df['O-3'] = df['Open'].shift(3)\n",
    "#     df['O-4'] = df['Open'].shift(4)\n",
    "#     df['O-5'] = df['Open'].shift(5)\n",
    "    \n",
    "#     df['c-1'] = df['Close'].shift(1)\n",
    "#     df['c-2'] = df['Close'].shift(2)\n",
    "#     df['c-3'] = df['Close'].shift(3)\n",
    "#     df['c-4'] = df['Close'].shift(4)\n",
    "#     df['c-5'] = df['Close'].shift(5)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #print(len(df))\n",
    "\n",
    "    \n",
    "    if intra ==True:\n",
    "            print(df.tail(1)['Datetime'])\n",
    "            old = df.tail(1).loc[:,'Open'].to_numpy()[0]\n",
    "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "            \n",
    "            \n",
    "            df['day_of_the_week'] = df['Datetime'].dt.dayofweek\n",
    "            df['day_of_month'] = df['Datetime'].apply(lambda x:int(str(x)[8:10]))\n",
    "            df['Datetime']=df['Datetime'].map(dt.datetime.toordinal)\n",
    "            \n",
    "    \n",
    "    elif intra ==False:\n",
    "            print(df.tail(1)['Date'])\n",
    "            old = df.tail(1).loc[:,'Open'].to_numpy()[0]\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            df['day_of_the_week'] = df['Date'].dt.dayofweek\n",
    "            df['day_of_month'] = df['Date'].apply(lambda x:int(str(x)[8:10]))\n",
    "            df['Date']=df['Date'].map(dt.datetime.toordinal)\n",
    "            \n",
    "            #df['SMA_1'] = df['Adj Close'].rolling(window = 50).mean()\n",
    "#             df['SMA_2'] = df['Adj Close'].rolling(window = 100).mean()\n",
    "\n",
    "\n",
    "     \n",
    "    df['High'] = df['High'].shift(1)\n",
    "    df['Low'] = df['Low'].shift(1)\n",
    "    df['Volume'] = df['Volume'].shift(1) \n",
    "    df['Middle Band'] =df['Adj Close'].rolling(window=26).mean()\n",
    "    df['Upper Band'] = df['Middle Band'] + 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['Lower Band'] = df['Middle Band'] - 1.96*df['Adj Close'].rolling(window=20).std()\n",
    "    df['status_lower'] = np.where(df['Adj Close'] < df['Lower Band'],'below_ballinger','normal')\n",
    "    df['status_upper'] = np.where(df['Adj Close'] > df['Upper Band'],'above_ballinger','normal')\n",
    "    df = df.dropna()\n",
    "\n",
    "    df = df.drop(columns = ['Close','tomorrow_closing'])\n",
    "    y = df['go_up']\n",
    "    X = df.drop(columns =['go_up','Adj Close','Volume' ,'Low','High'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle= False, random_state = 2020)\n",
    "        # first transformer for the numeric features\n",
    "    numeric_features = list(set(list(X_train.columns)) - set(['status_lower','status_upper','status_sma_lower','status_sma_upper']))\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "       ])\n",
    "    # now a taransformer for the categorical features\n",
    "    categorical_features = ['status_lower']\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    # creating a preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    ridge_mode = RandomForestClassifier(random_state = 2020, n_estimators = 2, n_jobs = 24)\n",
    "    # include the preprocessor and the model in one pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    reg_pipelin = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('Regressor', ridge_mode)])\n",
    "\n",
    "    # finally we will pass the pipe line to gridsearchcv to find the optimum paramters for the model\n",
    "#     param_grid ={'Regressor__bootstrap': [False], 'Regressor__max_depth': [10,20,50],\n",
    "#                  'Regressor__min_samples_leaf': [1,4,6], 'Regressor__min_samples_split': [5,10],\n",
    "#                  'Regressor__n_estimators': [10,50]}\n",
    "    \n",
    "    param_grid ={'Regressor__n_estimators': [200],\n",
    "                 'Regressor__max_depth': [15]\n",
    "                \n",
    "                }\n",
    "    \n",
    "    \n",
    "    search = GridSearchCV(reg_pipelin,param_grid,cv = 5)\n",
    "\n",
    "    # fitting the model\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # printing the first parameter\n",
    "    print(search.best_params_)\n",
    "    print(\"model training score: %.3f\" % search.score(X_train, y_train))\n",
    "    print(\"model test score: %.3f\" % search.score(X_test, y_test))\n",
    "    \n",
    "    # training the model on the whole data set\n",
    "\n",
    "    #search.fit(X_train,y_train)\n",
    "    y_pred = search.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print('accuracy = ', accuracy_score(y_test,y_pred),'precision = ', precision_score(y_test,y_pred),'recall = ', recall_score(y_test,y_pred))\n",
    "    \n",
    "#     error = np.sqrt(mean_squared_error(y_pred,y_test))\n",
    "#     print('error',error)\n",
    "# #     X_train = X[:-1]\n",
    "# #     y_train = y[:-1]\n",
    "#     X_test = X[-1:]\n",
    "#     y_test = y[-1:]\n",
    "#     y_test = search.predict(X_test)[0]\n",
    "#     diff = y_test - old\n",
    "        \n",
    "#     if diff > error :\n",
    "#         print('winner')\n",
    "#         print('old price = ',old)\n",
    "#         print('predicted = ',y_test)\n",
    "#         print('difference = ', diff)\n",
    "#     print('old price = ',old)\n",
    "#     print('predicted = ',y_test)\n",
    "#     print('difference = ', diff)\n",
    "    \n",
    "    #return error\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "6414   2020-11-25 15:20:00-05:00\n",
      "Name: Datetime, dtype: datetime64[ns, America/New_York]\n",
      "{'Regressor__max_depth': 15, 'Regressor__n_estimators': 200}\n",
      "model training score: 0.996\n",
      "model test score: 0.499\n",
      "[[614 171]\n",
      " [626 181]]\n",
      "accuracy =  0.4993718592964824 precision =  0.5142045454545454 recall =  0.22428748451053285\n"
     ]
    }
   ],
   "source": [
    "df_2 = read_data(stock='AMD',interval = \"2m\").reset_index()\n",
    "test_stock_grad_classify(df_2,intra = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "364   2020-11-24\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "{'Regressor__max_depth': 15, 'Regressor__n_estimators': 200}\n",
      "model training score: 1.000\n",
      "model test score: 0.471\n",
      "[[38  0]\n",
      " [45  2]]\n",
      "accuracy =  0.47058823529411764 precision =  1.0 recall =  0.0425531914893617\n"
     ]
    }
   ],
   "source": [
    "df_360 = read_data(stock='AAPL',period = \"366d\",interval = \"1d\").reset_index()\n",
    "test_stock_grad_classify(df_360,intra = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn =38\n",
    "# tp = 2\n",
    "\n",
    "# fp = 0\n",
    "# fn =1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason when I include the 50 sma , I got most of the postives there, high recall, but with low precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
